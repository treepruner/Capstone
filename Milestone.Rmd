---
title: "Millestone Report"
author: "Sarah Delaney"
date: "4/22/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


### Synopsis
This is the Milestone Report for the Coursera Data Scientist Capstone project to produce a word prediction application. The project has 3 sources of text information: 
* en_US.blogs.txt
* en_US.news.txt
* en_US.twitter.txt

The news file is written in standard, business English from an objective viewpoint. The blog file is more conversational English centered around personal experiences. The twitter file is filled with slang, abbreviations, profanity, emojois that in some cases may only be understood by the twitter user's followers. 

As a result of the differing adherence to standard English in the reference texts, the plan is to weight word associations in the news file the highest, the blog file lower and to perhaps ignore the twitter file completely. 

Specific profanity identified by the 458 "badwords" from Google will be replaced with the literal string 'profanity'.



```{r loadPackages, echo = TRUE, message = FALSE}
rm(list=(ls()))

require(RCurl)
require(readr)
require(profr)
require(dplyr)
require(sqldf)
require(quanteda)
require(ggplot2)

```

### Data Processing

####  Get External Data 
Download the external files with RCurl. 

```{r getData, echo = TRUE}

# get profanity reference file
if (!file.exists("./data/badwords.txt")){ 
profanityURL <- "https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/badwordslist/badwords.txt"
download.file(profanityURL, "./data/badwords.txt", method = "curl")}

# get main data
if (!file.exists("./data/Coursera-SwiftKey.zip")) { 
fileURL <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
download.file(fileURL, "./data/Coursera-SwiftKey.zip", method = "curl")}

if (!file.exists("./final/en_US/en_US.blogs.txt")) { unzip(zipfile = "./data/Coursera-SwiftKey.zip")}
fileListing <- list.files("./final/en_US")
print(fileListing)  


```


####  Load Data 
Initial load with UTF-8 encoding and skipped nulls. 
Record counts match examination of the files with Notepad++.
readr package read_lines required for news file's nulls.

```{r loadProfanity, echo = TRUE}

# load profanity reference file
profanity <- readLines("./data/badwords.txt")
profanity <- tolower(profanity)
profanity_rowCnt <- length(profanity) 

```




```{r loadProfanity, echo = TRUE}

# news
news <- read_lines("./final/en_US/en_US.news.txt")
news_rowCnt <-length(news) # 1,010,242

news_corpus <- corpus(news)

news_tokens <- tokens(news_corpus
                , remove_punct = TRUE 
                , remove_numbers = TRUE) %>% 
        tokens_select( stopwords('english')
                , selection = 'remove') %>%
        tokens_tolower() %>%
        tokens_replace(profanity, lemma, valuetype="fixed") %>%
        tokens_wordstem() 

rm("news")

```

```{r loadBlogs, echo = TRUE}

# blogs
blogs <- readLines("./final/en_US/en_US.blogs.txt", encoding = "UTF-8", skipNul = TRUE) 
blogs_rowCnt <- length(blogs)  # 899,288

blogs_corpus <- corpus(blogs)

blogs_tokens <- tokens(blogs_corpus
                , remove_punct = TRUE 
                , remove_numbers = TRUE) %>% 
        tokens_select( stopwords('english')
                , selection = 'remove') %>%
        tokens_tolower() %>%
        tokens_replace(profanity, lemma, valuetype="fixed") %>%
        tokens_wordstem() 

rm("blogs")

```




```{r loadTwitter, echo = TRUE}
# twitter
twitter <- readLines("./final/en_US/en_US.twitter.txt", encoding = "UTF-8", skipNul = TRUE)  # 
twitter_rowCnt <-length(twitter) # 2,360,148

twitter_corpus <- corpus(twitter)                

twitter_tokens <- tokens(twitter_corpus
                , remove_punct = TRUE 
                , remove_numbers = TRUE) %>% 
        tokens_select( stopwords('english')
                , selection = 'remove') %>%
        tokens_tolower() %>%
        tokens_replace(profanity, lemma, valuetype="fixed") %>%
        tokens_wordstem() 

rm("twitter")

```

# save originals if they don't exist
if (!file.exists("./data/blogs.rd")) { saveRDS(blogs, "./data/blogs.rds")}
if (!file.exists("./data/news.rd")) { saveRDS(news, "./data/news.rds") }
if (!file.exists("./data/twitter.rd")) {saveRDS(twitter, "./data/twitter.rds") }


# track rowcounts
fileNames <- c("profanity", "blogs", "news", "twitter")
initial_rowCnts <- c(profanity_rowCnt, blogs_rowCnt, news_rowCnt, twitter_rowCnt )
Control_Totals <- data.frame(fileNames, initial_rowCnts)
print(Control_Totals)



```




#### Create Corpus, clean, convert, and stem
Remove punctuation, numbers, and English stopwords, Convert to lowercase, replace any specific profade word with the literal "profanity" and then stem the words.

```{r createCoprpus, echo = TRUE}

# make specific profanity a generic literal
lemma <- rep("profanity", length(profanity))

# create corpus








```


#### Explore data
```{r exploreData, echo = TRUE}

ndoc(blogs_tokens)    # 899288        
head(blogs_tokens)
blogs_featureCnt <- nfeat(dfm(blogs_tokens)) # 300865


ndoc(news_tokens)    #     
head(news_tokens)
news_featureCnt <- nfeat(dfm(news_tokens)) # 

ndoc(twitter_tokens)    #     
head(twitter_tokens)
twitter_featureCnt <- nfeat(dfm(twitter_tokens)) # 

# Count Features and add to Control_Totals
featureCnts <- c(NA, blogs_featureCnt, news_featureCnt, twitter_featureCnt)
featureCntsDF <- data.frame(fileNames, featureCnts)

merge(Control_Totals, featureCntsDF)

```







## Including Plots

You can also embed plots, for example:

```{r Plots, echo=FALSE}

```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.



