---
title: "Millestone Report"
author: "Sarah Delaney"
date: "4/22/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


### Synopsis
This is the Milestone Report for the Coursera Data Scientist Capstone project to produce a word prediction application. The project has 3 sources of text information: 
*en_US.blogs.txt
*en_US.news.txt
*en_US.twitter.txt
*badwords.txt (from Google) 

The news file is written in standard, business English from an objective viewpoint. The blog file is more conversational English centered around personal experiences. The twitter file is filled with slang, abbreviations, profanity, and emojois. In some cases the twitter file might only be completely understood by the twitter user's followers. 

The individual files were each converted with the package Quanteda into a separate corpus- a file format suitable for text analysis.  Once in a corpus format, each file was cleaned, standardized and  separated into a tokens file that contains relevent word roots.

Transformations include:
*Removed punctuation, numbers, and English stopwords such as "a", "an" "the", etc 
*Changed to lowercase
*Replaced any specific 458 profane wordd with the literal "profanity"
*Stemmed the words to work with word roots

The tokens files were then converted into a document-feature-matrix format to support analysis of top occuring word roots. The workclouds are graphically interesting, but the frequency relationships are much more obvious with bar plots. For readability, only the top 25 word roots were plotted.  Unsurprisingly, the news file used the word root "said" significantly more than any other word.



```{r loadPackages, echo = TRUE, message = FALSE}
rm(list=(ls()))

require(RCurl)
require(readr)
require(profr)
require(dplyr)
require(tibble)
require(sqldf)
require(quanteda)
require(ggplot2)

```

### Data Processing

####  Get External Data 
Download the external files with RCurl. 

```{r getData, echo = FALSE}

# get profanity reference file
if (!file.exists("./data/badwords.txt")){ 
profanityURL <- "https://storage.googleapis.com/google-code-archive-downloads/v2/code.google.com/badwordslist/badwords.txt"
download.file(profanityURL, "./data/badwords.txt", method = "curl")}

# get main data
if (!file.exists("./data/Coursera-SwiftKey.zip")) { 
fileURL <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
download.file(fileURL, "./data/Coursera-SwiftKey.zip", method = "curl")}

if (!file.exists("./final/en_US/en_US.blogs.txt")) { unzip(zipfile = "./data/Coursera-SwiftKey.zip")}
fileListing <- list.files("./final/en_US")
print(fileListing)  


```


####  Load Data 
Load text files with UTF-8 encoding and skip nulls. 
Record counts matched examination of the files with Notepad++.

#### Profanity file 
Load "bad words" file from Google.

```{r loadProfanity, echo = FALSE}
# read in
profanity <- readLines("./data/badwords.txt", encoding = "UTF-8", skipNul = TRUE,  warn = FALSE)

# raw data examples and counts 
profanity_example <- head(profanity,2)
profanity_rowCnt <- length(profanity) 

# change to lowercase
profanity <- tolower(profanity)

# make specific profanity a generic literal
lemma <- rep("profanity", length(profanity))

# examples
print("Profanity file original" )
print(profanity_example)



```



## News File
readr package's read_lines function required for the news file.

```{r loadNews, echo = FALSE}
# readr package read_lines required for news file's nulls.

# read in
news <- read_lines("./final/en_US/en_US.news.txt")

# raw data examples and counts 
news_example <- as.data.frame( head(news,1,))
news_rowCnt <-length(news) # 1,010,242

# Corpus and cleaning
news_corpus <- corpus(news)

news_tokens <- tokens(news_corpus
                , remove_punct = TRUE 
                , remove_numbers = TRUE) %>% 
        tokens_select( stopwords('english')
                , selection = 'remove') %>%
        tokens_tolower() %>%
        tokens_replace(profanity, lemma, valuetype="fixed") %>%
        tokens_wordstem() 

# capture corpus examples and counts
news_tokens_example <- head(news_tokens,1)
news_tokensCnt <- ndoc(news_tokens)    # 1,010,242    
news_featureCnt <- nfeat(dfm(news_tokens)) # 274,774

# examples
print("News file original and tokens" )
print(news_example)
print(news_tokens_example)

# counts
Labels <- c("Source", "Original Rows", "Tokens Counted", "Features (Words) Counted")
news_Counts <- c("news", news_rowCnt, news_tokensCnt, news_featureCnt )
news_controlTotals <- data.frame(Labels, news_Counts)
print(news_controlTotals)

# save originals if they don't exist
if (!file.exists("./data/news.rds")) { saveRDS(news, "./data/news.rds") }

if (!file.exists("./data/news_tokens.rds")) { saveRDS(news_tokens, "./data/news_tokens.rds") }

# close raw file
rm("news")

# generate a DFM
news_dfm <- dfm(news_tokens)

# frequency plot for top 25
news_Top_25_dfm <- textstat_frequency(news_dfm, n = 25)
news_Top_25_dfm$feature <- with(news_Top_25_dfm, reorder(feature, frequency))
ggplot(news_Top_25_dfm, aes(y = feature, x = frequency)) +
        geom_point() +
        labs( title = "News", subtitle = "Top 25 Word Roots") +
        theme(axis.text.x = element_text(angle = 90, hjust = 1) )

# wordcloud
textplot_wordcloud(news_dfm)

# close big files
rm("news_tokens", "news_dfm")

# Preserve freq
news_Top_25_dfm <- news_Top_25_dfm %>% 
        mutate (group = "news")
print(news_Top_25_dfm)


```
## Blogs
The word "one" is the most frequently occuring word root in the blogs file.

```{r loadBlogs, echo = FALSE}

# read in
blogs <- readLines("./final/en_US/en_US.blogs.txt", encoding = "UTF-8", skipNul = TRUE) 

# raw data examples and counts
blogs_example <-  as.data.frame( head(blogs,1))
blogs_rowCnt <- length(blogs)  # 899,288

# Corpus and cleaning
blogs_corpus <- corpus(blogs)

blogs_tokens <- tokens(blogs_corpus
                , remove_punct = TRUE 
                , remove_numbers = TRUE) %>% 
        tokens_select( stopwords('english')
                , selection = 'remove') %>%
        tokens_tolower() %>%
        tokens_replace(profanity, lemma, valuetype="fixed") %>%
        tokens_wordstem() 

# capture corpus examples and counts
blogs_tokens_example <- head(blogs_tokens,1)  
blogs_tokensCnt <- ndoc(blogs_tokens)    #  899288   
blogs_featureCnt <- nfeat(dfm(blogs_tokens)) # 300865

# examples
print("Blogs file original and tokens" )
print(blogs_example)
print(blogs_tokens_example)

# counts
Labels <- c("Source", "Original Rows", "Tokens Counted", "Features (Words) Counted")
blogs_Counts <- c("blogs", blogs_rowCnt, blogs_tokensCnt, blogs_featureCnt )
blogs_controlTotals <- data.frame(Labels, blogs_Counts)
print(blogs_controlTotals)


# save originals if they don't exist
if (!file.exists("./data/blogs.rds")) { saveRDS(blogs, "./data/blogs.rds")}

if (!file.exists("./data/blogs_tokens.rds")) { saveRDS(blogs_tokens, "./data/blogs_tokens.rds") }

# close raw file
rm("blogs")


# generate a DFM
blogs_dfm <- dfm(blogs_tokens)

# frequency plot for top 25
blogs_Top_25_dfm <- textstat_frequency(blogs_dfm, n = 25)
blogs_Top_25_dfm$feature <- with(blogs_Top_25_dfm, reorder(feature, frequency))
ggplot(blogs_Top_25_dfm, aes(y = feature, x = frequency)) +
        geom_point() +
        labs( title = "blogs", subtitle = "Top 25 Word Roots") +
        theme(axis.text.x = element_text(angle = 90, hjust = 1) )

# wordcloud
textplot_wordcloud(blogs_dfm)

# close big files
rm("blogs_tokens", "blogs_dfm")

# Preserve freq
blogs_Top_25_dfm <- blogs_Top_25_dfm %>% 
        mutate (group = "blogs")
print(blogs_Top_25_dfm)

```



## Twitter
"just" is the most frequently occurring word in the twitter file and the 8th most frequently occurring is some form of profanity!
```{r loadTwitter, echo = FALSE}

# read in
twitter <- readLines("./final/en_US/en_US.twitter.txt", encoding = "UTF-8", skipNul = TRUE)   

# raw data examples and counts
twitter_example <-  as.data.frame( head(twitter,1))
twitter_rowCnt <-length(twitter) # 2,360,148


# Corpus and cleaning
# Remove punctuation, numbers, and English stopwords, 
# Change to lowercase
# Replace any specific profade word with the literal "profanity"
# Stem the words.

twitter_corpus <- corpus(twitter)                

twitter_tokens <- tokens(twitter_corpus
                , remove_punct = TRUE 
                , remove_numbers = TRUE) %>% 
        tokens_select( stopwords('english')
                , selection = 'remove') %>%
        tokens_tolower() %>%
        tokens_replace(profanity, lemma, valuetype="fixed") %>%
        tokens_wordstem() 

# capture corpus examples and counts
twitter_tokens_example <- head(twitter_tokens,1)  
twitter_tokensCnt <- ndoc(twitter_tokens)    #     
twitter_featureCnt <- nfeat(dfm(twitter_tokens)) # 

# examples
print("Twitters file original and tokens" )
print(twitter_example)
print(twitter_tokens_example)

# counts
Labels <- c("Source", "Original Rows", "Tokens Counted", "Features (Words) Counted")
twitter_Counts <- c("twitter", twitter_rowCnt, twitter_tokensCnt, twitter_featureCnt )
twitter_controlTotals <- data.frame(Labels, twitter_Counts)
print(twitter_controlTotals)


# save originals if they don't exist
if (!file.exists("./data/twitter.rds")) {saveRDS(twitter, "./data/twitter.rds") }

if (!file.exists("./data/twitter_tokens.rds")) { saveRDS(twitter_tokens, "./data/twitter_tokens.rds") }

# close raw file
rm("twitter")

# generate a DFM
twitter_dfm <- dfm(twitter_tokens)

# frequency plot for top 25
twitter_Top_25_dfm <- textstat_frequency(twitter_dfm, n = 25)
twitter_Top_25_dfm$feature <- with(twitter_Top_25_dfm, reorder(feature, frequency))
ggplot(twitter_Top_25_dfm, aes(y = feature, x = frequency)) +
        geom_point() +
        labs( title = "twitter File", subtitle = "Top 25 Word Roots") +
        theme(axis.text.x = element_text(angle = 90, hjust = 1) )

# wordcloud
textplot_wordcloud(twitter_dfm)

# close big files
rm("twitter_tokens", "twitter_dfm")

# Preserve freq
twitter_Top_25_dfm <- twitter_Top_25_dfm %>% 
        mutate (group = "twitter")
print(twitter_Top_25_dfm)

```

## Comparison of Top 25 Word Roots Across Files
The chart below shows all the top 25 ranked word roots in descending rank. There is some overlap between files, but each file has a different most frequently occurring word root. For example, the word "said" is the #1 ranked word root in the news file, but it does not appear in the list of top 25 word roots in the other files and no dot is plotted.
 
```{r loadTwitter, echo = FALSE}
All_Top_25_dfm <- rbind(news_Top_25_dfm, blogs_Top_25_dfm, twitter_Top_25_dfm) 
All_Top_25_dfm$feature <- with(All_Top_25_dfm, reorder(feature, -rank))

ggplot(All_Top_25_dfm, aes(y = feature, x = frequency)) +
        geom_point( aes(colour = rank)) +
        facet_grid(.~ group) +
        labs( title = "All", subtitle = "Top 25 Word Roots by Rank") +
        theme(axis.text.x = element_text(angle = 90, hjust = 1) )

```


